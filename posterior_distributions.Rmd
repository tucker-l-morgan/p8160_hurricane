---
title: "P8160 - Hurricane Project"
author: "Tucker Morgan - tlm2152"
date: "4/26/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Given Information

The following Bayesian model was suggested.  

$$Y_{i}(t+6) =\beta_{0,i}+\beta_{1,i}Y_{i}(t) + \beta_{2,i}\Delta_{i,1}(t)+
\beta_{3,i}\Delta_{i,2}(t) +\beta_{4,i}\Delta_{i,3}(t)  + \epsilon_{i}(t)$$  

where $Y_{i}(t)$ the wind speed at time $t$ (i.e. 6 hours earlier),  $\Delta_{i,1}(t)$, $\Delta_{i,2}(t)$ and $\Delta_{i,3}(t)$ are the changes of latitude, longitude and wind speed between $t$ and $t+6$, and $\epsilon_{i,t}$ follows a  normal distributions with mean zero and variance $\sigma^2$, independent across $t$. 

In the model,  $\boldsymbol{\beta}_{i} =  (\beta_{0,i},\beta_{1,i},...,\beta_{7,i})$ are the random coefficients associated the $i$th hurricane, we assume that 

$$\boldsymbol{\beta}_{i} \sim N(\boldsymbol{\beta}, \boldsymbol{\Sigma})$$

follows a multivariate normal distributions with mean $\boldsymbol{\beta}$ and covariance matrix $\Sigma$.

We assume the following non-informative or weak prior distributions for $\sigma^2$, $\boldsymbol{\beta}$ and $\Sigma$.

$$P(\sigma^2) \propto \frac{1}{\sigma^2};\quad P(\boldsymbol{\beta})\propto 1;\quad P(\Sigma^{-1}) \propto 
|\Sigma|^{-(d+1)} \exp(-\frac{1}{2}\Sigma^{-1})$$

$d$ is dimension of $\beta$.

\newpage

## Task 1:
Let $\textbf{B} = (\boldsymbol{\beta}_1^\top,..., \boldsymbol{\beta}_n^\top)^\top$, derive the posterior distribution of the parameters $\Theta = (\textbf{B}^\top, \boldsymbol{\beta}^\top, \sigma^2, \Sigma)$.

Note from given Bayesian model:

$$\epsilon_i(t) = Y_i(t+6) - \Big(\beta_{0,i} + \beta_{1,i}Y_i(t) + \beta_{2,i}\Delta_{i,1}(t) + \beta_{3,i}\Delta_{i,2}(t) + \beta_{4,i}\Delta_{i,3}(t)\Big) \stackrel{i.i.d}{\sim} N(0, \sigma^2)$$
$$\text{or}$$
$$Y_i(t+6) \stackrel{i.i.d}{\sim} N(\boldsymbol{X}_i^\top\boldsymbol{\beta}_i, \sigma^2)$$
where $\boldsymbol{X}_i^\top\boldsymbol{\beta}_i = \beta_{0,i} + \beta_{1,i}Y_i(t) + \beta_{2,1}\Delta_{i,1}(t) + \beta_{3,i}\Delta_{i,2}(t) + \beta_{4,i}\Delta_{i,3}(t)$. Therefore,

$$f_{Y_i(t+6)}(Y_i \mid \boldsymbol{\beta}_i,  \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\Big\{-\frac{1}{2\sigma^2}\Big[Y_i(t+6) - \Big(\beta_{0,i} + \beta_{1,i}Y_i(t) + \beta_{2,i}\Delta_{i,1}(t) + \beta_{3,i}\Delta_{i,2}(t) + \beta_{4,i}\Delta_{i,3}(t)\Big)\Big]^2 \Big\}$$
for hurricane $i$ at time $t$. To show the likelihood function for hurricane $i$ across all time points, $t$, we can write
$$f_{Y_i}(Y_i \mid \boldsymbol{\beta}_i, \sigma^2) \sim MVN(, \sigma^2 I)$$
(Need to find mean here by matrix multiplication of covariate matrix X and beta coefficients and update derivations below)

We can find the posterior distribution for $\Theta$ by

$$\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i) \propto f_{Y_i(t+6)}(Y_i \mid \boldsymbol{\beta},  \sigma^2) \times \pi(\textbf{B} \mid \boldsymbol{\beta}, \Sigma) \times \pi(\boldsymbol{\beta}) \times \pi(\sigma^2) \times \pi(\Sigma),$$

where $\pi(\textbf{B} \mid \boldsymbol{\beta},  \Sigma)$ is the joint multivariate normal density of $\beta$,

$$\pi(\textbf{B} \mid \boldsymbol{\beta},  \Sigma) = \prod_{i=1}^n \det(2\pi\Sigma)^{-\frac{1}{2}} \exp(-\frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top \Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta})).$$

So we have the following posterior distribution, from which we can derive conditional posteriors to perform Gibbs sampling.

$$\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i) \propto \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\Big\{-\frac{1}{2\sigma^2}\Big(Y_i(t+6) - \boldsymbol{X}_i^\top\boldsymbol{\beta}_i\Big)^2 \Big\}\\$$
$$\times \prod_{i=1}^n \det(2\pi\Sigma)^{-\frac{1}{2}} \exp\Big(-\frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top \Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta})\Big) \times \frac{1}{\sigma^2} \times |\Sigma|^{-(d+1)} \exp(-\frac{1}{2}\Sigma^{-1})$$

Rearranged,
$$\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i) \propto \frac{\det(2\pi\Sigma)^{-1/2}}{\sqrt{2\pi}\sigma^3} | \Sigma |^{-(d + 1)}\exp\Big(-\frac{1}{2}\Sigma^{-1}\Big) \prod_{i=1}^n \exp\Big\{-\frac{1}{2\sigma^2}(Y_i(t+6) - \boldsymbol{X}_i^\top\boldsymbol{\beta}_i)^2 - \frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top\Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta})\Big\}$$
And therefore,
$$\log{\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i)} \propto \sum_{i=1}^n -\frac{1}{2}\log2\pi - \log \sigma - \frac{1}{2\sigma^2}\Big(Y_i(t + 6) - \boldsymbol{X}_i^\top\boldsymbol{\beta}_i \Big)^2 +$$
$$\sum_{i=1}^n -\frac{1}{2}\log(\det(2\pi\Sigma)) - \frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top \Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta}) - 2\log\sigma - (d+1)\log|\Sigma| - \frac{1}{2}\Sigma^{-1}.$$


To find the conditional posterior distributions for Gibbs sampling, we can consider:
$$\pi(\boldsymbol{\beta} \mid \textbf{B}, \sigma^2, \Sigma, Y_i) = \frac{\pi(\beta, \textbf{B},\sigma^2, \Sigma \mid Y_i)}{\pi(\textbf{B}, \sigma^2, \Sigma)} \propto\ ??$$

In the equation above, $\pi(\textbf{B}, \sigma^2, \Sigma)$ only depends on $\textbf{B}, \sigma^2, \Sigma$, so we can focus only on the part of the joint posterior that depends on all four variables $\beta,\ \textbf{B},\ \sigma^2$, and $\Sigma$.

$$\pi(\boldsymbol{\beta} \mid \textbf{B}, \sigma^2, \Sigma, Y_i) \propto \frac{1}{\sigma^2} \exp\Big\{- \frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top\Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta})\Big\}$$
$$\propto \frac{1}{\sigma^2}\exp\Big\{-\frac{1}{2}\Big(\boldsymbol{\beta}_i^\top\Sigma^{-1}\boldsymbol{\beta}_i - \boldsymbol{\beta_i}^\top \Sigma^{-1}\boldsymbol{\beta} - \boldsymbol{\beta}^\top\Sigma^{-1}\boldsymbol{\beta}_i - \boldsymbol{\beta}^\top\Sigma^{-1}\boldsymbol{\beta}\Big)\Big\}$$
$$\propto \frac{1}{\sigma^2}\exp\Big\{\boldsymbol{\beta}^\top\Sigma^{-1}\boldsymbol{\beta}_i - \frac{1}{2}\boldsymbol{\beta}^\top\Sigma^{-1}\boldsymbol{\beta}\Big\}$$
I believe we want to expand and simplify to find a recognizable distribution, but I'm not sure what to do from here. Seems like I've made an error somewhere? Would it be easier to do Metropolis-Hastings?

\newpage

The log of the posterior can be written as:
$$\log{\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i)} \propto \log f_{Y_i(t + 6)}(Y_i \mid \boldsymbol{\beta}_i, \sigma^2) + \log \pi(\Theta),$$

where

$$\pi(\Theta) \propto \prod_{i=1}^n \det(2\pi\Sigma)^{-\frac{1}{2}} \exp(-\frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top \Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta})) \times 1 \times \frac{1}{\sigma^2} \times |\Sigma|^{-(d+1)} \exp(-\frac{1}{2}\Sigma^{-1}).$$
Therefore,
$$\log{\pi(\textbf{B}, \boldsymbol{\beta}, \sigma^2, \Sigma \mid Y_i)} \propto \sum_{i=1}^n -\frac{1}{2}\log2\pi - \log \sigma - \frac{1}{2\sigma^2}\Big(Y_i(t + 6) - \mu_i \Big)^2 +$$
$$\sum_{i=1}^n -\frac{1}{2}\log(\det(2\pi\Sigma)) - \frac{1}{2}(\boldsymbol{\beta}_i - \boldsymbol{\beta})^\top \Sigma^{-1}(\boldsymbol{\beta}_i - \boldsymbol{\beta}) - 2\log\sigma - (d+1)\log|\Sigma| - \frac{1}{2}\Sigma^{-1}.$$

Using this equation, I think we could implement a similar component-wise M-H algorithm to the Example: Hierarchical Poisson Model from lecture note 9, page 45.